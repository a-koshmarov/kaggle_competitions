{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kaggle_simpsons","version":"0.3.2","provenance":[{"file_id":"1u6pK4XOth0HSSaJ0eyQv-WILR8nJ-pTr","timestamp":1557154223740}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Xw7YkEefehWo","colab_type":"text"},"source":["# Путешествие по Спрингфилду.\n","\n","\n","Сегодня вам предстоить помочь телекомпании FOX  в обработке их контента. Как вы знаете сериал Симсоны идет на телеэкранах более 25 лет и за это время скопилось очень много видео материала. Персоонажи менялись вместе с изменяющимися графическими технологиями   и Гомер 2018 не очень похож на Гомера 1989. Нашей задачей будет научиться классифицировать персонажей проживающих в Спрингфилде. Думаю, что нет смысла представлять каждого из них в отдельности.\n","\n","\n","\n"," ![alt text](https://vignette.wikia.nocookie.net/simpsons/images/5/5a/Spider_fat_piglet.png/revision/latest/scale-to-width-down/640?cb=20111118140828)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aPxcEtJepqir","colab_type":"text"},"source":["Скрипт для аугментации данных, которые используются для обучения сети. Необходимо запускать скрипт локально и загружать его с google диска."]},{"cell_type":"code","metadata":{"id":"N_RUO8QypzcN","colab_type":"code","colab":{}},"source":["import operator\n","import os\n","import Augmentor\n","import numpy as np\n","\n","\n","def augment(train_dir):\n","    files = os.listdir(train_dir)\n","    print(len(files))\n","    images = {}\n","    names = []\n","    for name in files:\n","        print('{} : {}'.format(name, len([x for x in os.listdir(os.path.join(train_dir, name))])))\n","        count = len([x for x in os.listdir(os.path.join(train_dir, name))])\n","\n","        if count != 0 and count < 1000:\n","            p = Augmentor.Pipeline(train_dir + name)\n","\n","            p.random_distortion(probability=0.5, grid_width=100, grid_height=100, magnitude=1)\n","            p.random_erasing(probability=0.5, rectangle_area=0.2)\n","            p.rotate(probability=0.5, max_left_rotation=25, max_right_rotation=25)\n","            p.random_color(probability=1.0, min_factor=0.5, max_factor=1.5)\n","            p.flip_random(probability=0.5)\n","            p.crop_random(probability=0.5, percentage_area=0.8)\n","            p.resize(probability=1.0, width=224, height=224)\n","\n","            if count < 300:\n","                p.sample(300 - count)\n","            elif count < 500:\n","                p.sample(500 - count)\n","            elif count < 800:\n","                p.sample(800 - count)\n","            else:\n","                p.sample(1000 - count)\n","\n","            p.sample(21)\n","            names.append(name)\n","\n","        images[name] = count\n","\n","    max_value = max(images.items(), key=operator.itemgetter(1))[0]\n","    print('max number of images is {} : {}'.format(max_value, images[max_value]))\n","\n","    names = np.array(names)\n","    print('names with less than 1000 images: {}'.format(names))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xA3o2xC3MlMI","colab_type":"code","outputId":"ebc8af2b-df68-441d-da68-c37c47b9b7eb","executionInfo":{"status":"ok","timestamp":1557326929082,"user_tz":-180,"elapsed":19885,"user":{"displayName":"Александр Комшанов","photoUrl":"https://lh5.googleusercontent.com/-u1x24Wdsmi4/AAAAAAAAAAI/AAAAAAAAAp8/HTTDaVIAMuI/s64/photo.jpg","userId":"11750318642303468228"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kRGt7YicMxYI","colab_type":"code","colab":{}},"source":["!unzip -q /content/gdrive/My\\ Drive/ml/week_9/simpsons_dataset.zip -d train\n","!unzip -q /content/gdrive/My\\ Drive/ml/week_9/testset.zip -d test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"naD6xsZzMxrC","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","import numpy as np\n","from skimage import io\n","\n","from tqdm import tqdm, tqdm_notebook\n","from PIL import Image\n","from pathlib import Path\n","\n","import torch\n","from torchvision import transforms\n","from multiprocessing.pool import ThreadPool\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torchvision.transforms.functional as F\n","\n","from matplotlib import colors, pyplot as plt\n","%matplotlib inline\n","\n","# в sklearn не все гладко, чтобы в colab удобно выводить картинки \n","# мы будем игнорировать warnings\n","import warnings\n","\n","\n","\n","warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n","# разные режимы датасета \n","DATA_MODES = ['train', 'val', 'test']\n","# все изображения будут масштабированы к размеру 224x224 px\n","RESCALE_SIZE = 224\n","# работаем на видеокарте\n","DEVICE = torch.device(\"cuda\")\n","\n","model_save_path=\"simpsons_classifier.pth\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cj32U5iTQUe4","colab_type":"code","colab":{}},"source":["class SimpsonsDataset(Dataset):\n","    \"\"\"\n","    Датасет с картинками, который паралельно подгружает их из папок\n","    производит скалирование и превращение в торчевые тензоры\n","    \"\"\"\n","    def __init__(self, files, mode):\n","        super().__init__()\n","        # список файлов для загрузки\n","        self.files = sorted(files)\n","        # режим работы\n","        self.mode = mode\n","\n","        if self.mode not in DATA_MODES:\n","            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n","            raise NameError\n","\n","        self.len_ = len(self.files)\n","     \n","        self.label_encoder = LabelEncoder()\n","\n","        if self.mode != 'test':\n","            self.labels = [path.parent.name for path in self.files]\n","            self.label_encoder.fit(self.labels)\n","\n","            with open('label_encoder.pkl', 'wb') as le_dump_file:\n","                  pickle.dump(self.label_encoder, le_dump_file)\n","                      \n","    def __len__(self):\n","        return self.len_\n","      \n","    def load_sample(self, file):\n","        image = Image.open(file)\n","        image.load()\n","        return image\n","  \n","    def __getitem__(self, index):\n","        # для преобразования изображений в тензоры PyTorch и нормализации входа\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n","        ])\n","        x = self.load_sample(self.files[index])\n","        x = self._prepare_sample(x)\n","        x = np.array(x / 255, dtype='float32')\n","        x = transform(x)\n","        if self.mode == 'test':\n","            return x\n","        else:\n","            label = self.labels[index]\n","            label_id = self.label_encoder.transform([label])\n","            y = label_id.item()\n","            return x, y\n","        \n","    def _prepare_sample(self, image):\n","        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n","        return np.array(image)\n","        \n","def imshow(inp, title=None, plt_ax=plt, default=False):\n","    \"\"\"Imshow для тензоров\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt_ax.imshow(inp)\n","    if title is not None:\n","        plt_ax.set_title(title)\n","    plt_ax.grid(False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUhzOq1zRJil","colab_type":"code","colab":{}},"source":["import operator\n","TRAIN_DIR = Path('train/simpsons_dataset')\n","TEST_DIR = Path('test/testset')\n","\n","files = os.listdir(TRAIN_DIR)\n","print(len(files))\n","images = {}\n","for name in files:\n","    print('{} : {}'.format(name, len([x for x in os.listdir(os.path.join(TRAIN_DIR, name))])))\n","    images[name] = len([x for x in os.listdir(os.path.join(TRAIN_DIR, name))])                    \n","\n","max_value = max(images.items(), key=operator.itemgetter(1))[0]\n","print('max number of images is {} : {}'.format(max_value, images[max_value]))\n","\n","train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n","test_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n","\n","print(train_val_files)\n","from sklearn.model_selection import train_test_split\n","\n","train_val_labels = [path.parent.name for path in train_val_files]\n","train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n","                                          stratify=train_val_labels)\n","\n","val_dataset = SimpsonsDataset(val_files, mode='val')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6YcZk8vQR47","colab_type":"text"},"source":["### Построение нейросети\n","\n","Архитектура - AlexNet\n","\n","*Описание слоев*:\n","\n","1. размерность входа: 3x224x224 \n","2. Conv (11x11, stride=4, 96 kernels)\n","3. MaxPool (3x3, stride=2)\n","4. Conv (5x5, padding=2, 256 kernels)\n","5. MaxPool (3x3, stride=2)\n","6. Conv (3x3, padding=1, 384 kernels)\n","7. Conv (3x3, padding=1, 384 kernels)\n","8. Conv (3x3, padding=1, 384 kernels)\n","9. MaxPool (3x3, stride=2)\n","10. FC (4096)\n","11. FC (4096)\n","12. FC (n_classes)\n","\n","Dropout rate = 0.5\n","lr = 3e-4\n","batch_size = 64\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j8zZHqo_-cTw","colab_type":"text"},"source":["### Модель обученная на аугментированных данных - [ссылка](https://drive.google.com/drive/folders/1eA22AyqtbbAsqFZUM9TbALmpFEU_EBql?usp=sharing) на Google диск с simpsons_classifier.pth\n"]},{"cell_type":"code","metadata":{"id":"1PJcWAhuji-i","colab_type":"code","colab":{}},"source":["class AlexNet(nn.Module):\n","  \n","  def __init__(self, n_classes):\n","        super().__init__()\n","        \n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),\n","            nn.BatchNorm2d(96),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        \n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU(),\n","        )\n","        \n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU(),\n","        )\n","        \n","        self.conv5 = nn.Sequential(\n","            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU(),\n","        )\n","        \n","        self.conv6 = nn.Sequential(\n","            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        \n","        self.fc1 = nn.Sequential(\n","            nn.Linear(256*5*5, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        self.out = nn.Linear(4096, n_classes)\n","        \n","  def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        x = self.conv6(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        logits = self.out(x)\n","        return logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2mk7MNtcUhJ","colab_type":"code","colab":{}},"source":["def fit_epoch(model, train_loader, criterion, optimizer):\n","    running_loss = 0.0\n","    running_corrects = 0\n","    processed_data = 0\n","  \n","    for inputs, labels in train_loader:\n","        inputs = inputs.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        preds = torch.argmax(outputs, 1)\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","        processed_data += inputs.size(0)\n","              \n","    train_loss = running_loss / processed_data\n","    train_acc = running_corrects.cpu().numpy() / processed_data\n","    return train_loss, train_acc\n","  \n","  \n","  \n","def eval_epoch(model, val_loader, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    processed_size = 0\n","\n","    for inputs, labels in val_loader:\n","        inputs = inputs.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            preds = torch.argmax(outputs, 1)\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","        processed_size += inputs.size(0)\n","    val_loss = running_loss / processed_size\n","    val_acc = running_corrects.double() / processed_size\n","    return val_loss, val_acc\n","  \n","  \n","  \n","def train(train_files, val_files, model, epochs, batch_size):\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    history = []\n","    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n","    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n","\n","    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n","        opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        for epoch in range(epochs):\n","            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n","            print(\"loss\", train_loss)\n","            \n","            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n","            history.append((train_loss, train_acc, val_loss, val_acc))\n","            \n","            pbar_outer.update(1)\n","            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n","                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n","            \n","    torch.save(model.state_dict(), model_save_path)\n","            \n","    return history\n","  \n","  \n","  \n","def predict(model, test_loader):\n","    with torch.no_grad():\n","        logits = []\n","    \n","        for inputs in test_loader:\n","            inputs = inputs.to(DEVICE)\n","            model.eval()\n","            outputs = model(inputs).cpu()\n","            logits.append(outputs)\n","            \n","    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n","    return probs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzwhB4K3dQOC","colab_type":"code","outputId":"f05489ee-8e0d-4e2e-c407-7fb7e994afa3","executionInfo":{"status":"ok","timestamp":1557333958362,"user_tz":-180,"elapsed":484,"user":{"displayName":"Александр Комшанов","photoUrl":"https://lh5.googleusercontent.com/-u1x24Wdsmi4/AAAAAAAAAAI/AAAAAAAAAp8/HTTDaVIAMuI/s64/photo.jpg","userId":"11750318642303468228"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"source":["n_classes = len(np.unique(train_val_labels))\n","simple_cnn = AlexNet(n_classes).to(DEVICE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["we will classify :42\n","SimpleCnn(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n","    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout2d(p=0.05)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n","    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout2d(p=0.1)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv3): Sequential(\n","    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout2d(p=0.15)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout2d(p=0.2)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv5): Sequential(\n","    (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1))\n","    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout2d(p=0.25)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fc1): Sequential(\n","    (0): Linear(in_features=2400, out_features=1024, bias=True)\n","    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout(p=0.4)\n","  )\n","  (out): Linear(in_features=1024, out_features=42, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bo3UND5RdgVg","colab_type":"text"},"source":["Запустим обучение сети."]},{"cell_type":"code","metadata":{"id":"WDkcxZ1kfD4a","colab_type":"code","colab":{}},"source":["if val_dataset is None:\n","    val_dataset = SimpsonsDataset(val_files, mode='val')\n","    \n","train_dataset = SimpsonsDataset(train_files, mode='train')\n","\n","\n","history = train(train_dataset, val_dataset, model=simple_cnn, epochs=20, batch_size=64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5k0UGeTNaQX","colab_type":"text"},"source":["Проверка на validation датасете"]},{"cell_type":"code","metadata":{"id":"Z8PlF6o0N9O1","colab_type":"code","colab":{}},"source":["def predict_one_sample(model, inputs, device=DEVICE):\n","    \"\"\"Предсказание, для одной картинки\"\"\"\n","    with torch.no_grad():\n","        inputs = inputs.to(device)\n","        model.eval()\n","        logit = model(inputs).cpu()\n","        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n","    return probs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pY_OoLoVO_9V","colab_type":"code","colab":{}},"source":["random_characters = int(np.random.uniform(0,1000))\n","ex_img, true_label = val_dataset[random_characters]\n","probs_im = predict_one_sample(simple_cnn, ex_img.unsqueeze(0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"caivVFeAN9SY","colab_type":"code","colab":{}},"source":["idxs = list(map(int, np.random.uniform(0,1000, 20)))\n","imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n","\n","probs_ims = predict(simple_cnn, imgs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-0pRdHnQQKM","colab_type":"code","colab":{}},"source":["label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNMFc7sfQh1a","colab_type":"code","colab":{}},"source":["y_pred = np.argmax(probs_ims,-1)\n","\n","actual_labels = [val_dataset[id][1] for id in idxs]\n","\n","preds_class = [label_encoder.classes_[i] for i in y_pred]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hO9OLOMqIXRV","colab_type":"text"},"source":["Сабмит на kaggle"]},{"cell_type":"code","metadata":{"id":"9UTbU0Zbc6Hb","colab_type":"code","colab":{}},"source":["test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n","probs = predict(simple_cnn, test_loader)\n","\n","\n","preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n","test_filenames = [path.name for path in test_dataset.files]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yw0zZ-Hdd89s","colab_type":"code","colab":{}},"source":["import pandas as pd\n","my_submit = pd.read_csv(\"gdrive/My Drive/simpsons/data/labels.csv\")\n","my_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\n","my_submit.head()\n","my_submit.to_csv('gdrive/My Drive/simpsons/simple_cnn_baseline.csv', index=False)"],"execution_count":0,"outputs":[]}]}